{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9APO91Z0OSJl9SD5ND7xa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdulAhadSiddiqui-0786/Voice-AI/blob/main/Voice_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KaeuccIyDLnB"
      },
      "outputs": [],
      "source": [
        "!pip install pytube pydub openai-whisper pyannote.audio torch torchvision torchaudio transformers -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp -q\n",
        "from pydub import AudioSegment\n",
        "\n",
        "url = \"https://www.youtube.com/watch?v=4ostqJD3Psc\"\n",
        "\n",
        "# Download audio with yt-dlp\n",
        "!yt-dlp -x --audio-format mp3 -o \"call.%(ext)s\" {url}\n",
        "\n",
        "# Convert to wav, mono, 16kHz\n",
        "audio = AudioSegment.from_file(\"call.mp3\")\n",
        "audio = audio.set_channels(1).set_frame_rate(16000)\n",
        "audio.export(\"call.wav\", format=\"wav\")\n",
        "\n",
        "print(\" Audio downloaded & converted to WAV using yt-dlp!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9cwj2BlDwhf",
        "outputId": "bee55e9e-aef1-448f-e061-71a7dad03788"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=4ostqJD3Psc\n",
            "[youtube] 4ostqJD3Psc: Downloading webpage\n",
            "[youtube] 4ostqJD3Psc: Downloading tv simply player API JSON\n",
            "[youtube] 4ostqJD3Psc: Downloading tv client config\n",
            "[youtube] 4ostqJD3Psc: Downloading player 0004de42-main\n",
            "[youtube] 4ostqJD3Psc: Downloading tv player API JSON\n",
            "[info] 4ostqJD3Psc: Downloading 1 format(s): 251\n",
            "[download] call.mp3 has already been downloaded\n",
            "[ExtractAudio] Not converting audio call.mp3; file is already in target format mp3\n",
            " Audio downloaded & converted to WAV using yt-dlp!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyannote.audio import Pipeline\n",
        "from google.colab import userdata\n",
        "from tqdm import tqdm\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "#  Hugging Face token\n",
        "HUGGINGFACE_TOKEN = userdata.get('HUGGINGFACE_TOKEN')\n",
        "\n",
        "# 1. Load diarization model\n",
        "print(\" Loading diarization model...\")\n",
        "pipeline = Pipeline.from_pretrained(\n",
        "    \"pyannote/speaker-diarization\",\n",
        "    use_auth_token=HUGGINGFACE_TOKEN\n",
        ")\n",
        "print(\" Model loaded!\")\n",
        "\n",
        "# 2. Load audio and get duration\n",
        "audio_file = \"call.wav\"\n",
        "info = sf.info(audio_file)\n",
        "duration = info.duration  # in seconds\n",
        "print(f\" Audio duration: {duration:.1f} sec\")\n",
        "\n",
        "# 3. Split into 30s chunks\n",
        "chunk_size = 30_000  # in ms\n",
        "audio = AudioSegment.from_wav(audio_file)\n",
        "chunks = math.ceil(len(audio) / chunk_size)\n",
        "print(f\" Splitting into {chunks} chunks of 30s each...\")\n",
        "\n",
        "# 4. Process chunks with caching\n",
        "speaker_segments = []\n",
        "with tqdm(total=chunks, desc=\"Processing chunks\") as pbar:\n",
        "    for i in range(chunks):\n",
        "        start = i * chunk_size\n",
        "        end = min((i + 1) * chunk_size, len(audio))\n",
        "        chunk_file = f\"chunk_{i}.wav\"\n",
        "        cache_file = f\"chunk_{i}_diarization.pkl\"\n",
        "\n",
        "        # Export chunk only if not exists\n",
        "        if not os.path.exists(chunk_file):\n",
        "            audio[start:end].export(chunk_file, format=\"wav\")\n",
        "\n",
        "        # Load from cache if exists\n",
        "        if os.path.exists(cache_file):\n",
        "            with open(cache_file, \"rb\") as f:\n",
        "                diarization = pickle.load(f)\n",
        "        else:\n",
        "            # Run diarization on this chunk\n",
        "            diarization = pipeline(chunk_file)\n",
        "            with open(cache_file, \"wb\") as f:\n",
        "                pickle.dump(diarization, f)\n",
        "\n",
        "        # Extract speaker segments\n",
        "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "            speaker_segments.append({\n",
        "                \"speaker\": speaker,\n",
        "                \"start\": turn.start + start / 1000,  # shift time\n",
        "                \"end\": turn.end + start / 1000,\n",
        "                \"duration\": turn.end - turn.start\n",
        "            })\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "print(\" Diarization complete!\")\n",
        "print(\" Speakers found:\", set([s[\"speaker\"] for s in speaker_segments]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0x2Vgv4EKds",
        "outputId": "20587e43-2b4f-4069-f282-baaa61361e5f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyannote/audio/core/io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  torchaudio.list_audio_backends()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loading diarization model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/speechbrain/utils/torch_audio_backend.py:57: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  available_backends = torchaudio.list_audio_backends()\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n",
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n",
            "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/hyperparams.yaml'\n",
            "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.8.0+cu126. Bad things might happen unless you revert torch to 1.x.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered parameter transfer hook for _load\n",
            "/usr/local/lib/python3.12/dist-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load_if_possible\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Collecting files (or symlinks) for pretraining in /root/.cache/torch/pyannote/speechbrain.\n",
            "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/embedding_model.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"embedding_model\"] = /root/.cache/torch/pyannote/speechbrain/embedding_model.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"mean_var_norm_emb\"] = /root/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/classifier.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"classifier\"] = /root/.cache/torch/pyannote/speechbrain/classifier.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"label_encoder\"] = /root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt\n",
            "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): embedding_model -> /root/.cache/torch/pyannote/speechbrain/embedding_model.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): mean_var_norm_emb -> /root/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): classifier -> /root/.cache/torch/pyannote/speechbrain/classifier.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): label_encoder -> /root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt\n",
            "DEBUG:speechbrain.dataio.encoder:Loaded categorical encoding from /root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model loaded!\n",
            " Audio duration: 122.7 sec\n",
            " Splitting into 5 chunks of 30s each...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing chunks: 100%|██████████| 5/5 [00:00<00:00, 3264.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Diarization complete!\n",
            " Speakers found: {'SPEAKER_02', 'SPEAKER_01', 'SPEAKER_04', 'SPEAKER_00', 'SPEAKER_03'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "# Load Whisper model\n",
        "model = whisper.load_model(\"small\")   # small model = good balance (fast + accurate)\n",
        "\n",
        "# Transcribe audio\n",
        "result = model.transcribe(\"call.wav\")\n",
        "transcript = result[\"text\"]\n",
        "\n",
        "print(\" Transcription complete!\")\n",
        "print(\"Sample transcript:\", transcript[:250], \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tg64H7oEST9",
        "outputId": "858409b6-d315-4b7d-ea57-763b7c9c6137"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Transcription complete!\n",
            "Sample transcript:  Thank you for calling Nissan. My name is Lauren. Can I have your name? Yeah, my name is John Smith. Thank you, John. How can I help you? I was just calling about to see how much it would cost to update the map in my car. I'd be happy to help you wit ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "# 1. Talk-time ratio\n",
        "talk_time = {}\n",
        "for seg in speaker_segments:\n",
        "    talk_time[seg[\"speaker\"]] = talk_time.get(seg[\"speaker\"], 0) + seg[\"duration\"]\n",
        "\n",
        "total_time = sum(talk_time.values())\n",
        "talk_ratios = {sp: round((dur/total_time)*100, 2) for sp, dur in talk_time.items()}\n",
        "\n",
        "# 2. Number of questions\n",
        "num_questions = transcript.count(\"?\")\n",
        "extra_questions = len(re.findall(r\"\\b(what|why|how|when|where|can|do|is|are|does|did)\\b\", transcript.lower()))\n",
        "question_count = max(num_questions, extra_questions)\n",
        "\n",
        "# 3. Longest monologue\n",
        "longest_monologue = max([seg[\"duration\"] for seg in speaker_segments])\n",
        "\n",
        "# 4. Sentiment\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "sentiment = sentiment_analyzer(transcript[:500])[0]  # analyze first 500 chars\n",
        "\n",
        "# 5. Actionable insight\n",
        "insight = \"\"\n",
        "if max(talk_ratios.values()) > 70:\n",
        "    insight = \"One speaker dominated the conversation. Allow more balanced talk-time.\"\n",
        "elif question_count < 3:\n",
        "    insight = \"Too few questions were asked. Ask more questions to engage the customer.\"\n",
        "else:\n",
        "    insight = \"Good balance, but could improve listening.\"\n",
        "\n",
        "# Identify roles\n",
        "sales_rep = max(talk_ratios, key=talk_ratios.get)\n",
        "customer = min(talk_ratios, key=talk_ratios.get)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ndsDMo0H63n",
        "outputId": "bb9af698-59c8-49c3-efc5-e753afc05104"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n===== CALL QUALITY REPORT =====\")\n",
        "print(\"Talk-time ratio:\", talk_ratios)\n",
        "print(\"Questions asked:\", question_count)\n",
        "print(\"Longest monologue (sec):\", round(longest_monologue, 2))\n",
        "print(\"Call sentiment:\", sentiment[\"label\"], \"| Confidence:\", round(sentiment[\"score\"], 2))\n",
        "print(\"Actionable Insight:\", insight)\n",
        "print(\"Likely Sales Rep:\", sales_rep, \"| Likely Customer:\", customer)\n"
      ],
      "metadata": {
        "id": "EGhEi3hTIEPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0bc0697-07f7-402f-eb33-dc695ff4f6d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== CALL QUALITY REPORT =====\n",
            "Talk-time ratio: {'SPEAKER_00': 33.52, 'SPEAKER_01': 32.02, 'SPEAKER_02': 23.03, 'SPEAKER_03': 10.52, 'SPEAKER_04': 0.91}\n",
            "Questions asked: 20\n",
            "Longest monologue (sec): 11.61\n",
            "Call sentiment: POSITIVE | Confidence: 1.0\n",
            "Actionable Insight: Good balance, but could improve listening.\n",
            "Likely Sales Rep: SPEAKER_00 | Likely Customer: SPEAKER_04\n"
          ]
        }
      ]
    }
  ]
}