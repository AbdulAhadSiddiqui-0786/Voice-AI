{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9APO91Z0OSJl9SD5ND7xa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdulAhadSiddiqui-0786/Voice-AI/blob/main/Voice_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaeuccIyDLnB"
      },
      "outputs": [],
      "source": [
        "!pip install pytube pydub openai-whisper pyannote.audio torch torchvision torchaudio transformers -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp -q\n",
        "from pydub import AudioSegment\n",
        "\n",
        "url = \"https://www.youtube.com/watch?v=4ostqJD3Psc\"\n",
        "\n",
        "# Download audio with yt-dlp\n",
        "!yt-dlp -x --audio-format mp3 -o \"call.%(ext)s\" {url}\n",
        "\n",
        "# Convert to wav, mono, 16kHz\n",
        "audio = AudioSegment.from_file(\"call.mp3\")\n",
        "audio = audio.set_channels(1).set_frame_rate(16000)\n",
        "audio.export(\"call.wav\", format=\"wav\")\n",
        "\n",
        "print(\" Audio downloaded & converted to WAV using yt-dlp!\")\n"
      ],
      "metadata": {
        "id": "k9cwj2BlDwhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyannote.audio import Pipeline\n",
        "from google.colab import userdata\n",
        "from tqdm import tqdm\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "#  Hugging Face token\n",
        "HUGGINGFACE_TOKEN = userdata.get('HUGGINGFACE_TOKEN')\n",
        "\n",
        "# 1. Load diarization model\n",
        "print(\" Loading diarization model...\")\n",
        "pipeline = Pipeline.from_pretrained(\n",
        "    \"pyannote/speaker-diarization\",\n",
        "    use_auth_token=HUGGINGFACE_TOKEN\n",
        ")\n",
        "print(\" Model loaded!\")\n",
        "\n",
        "# 2. Load audio and get duration\n",
        "audio_file = \"call.wav\"\n",
        "info = sf.info(audio_file)\n",
        "duration = info.duration  # in seconds\n",
        "print(f\" Audio duration: {duration:.1f} sec\")\n",
        "\n",
        "# 3. Split into 30s chunks\n",
        "chunk_size = 30_000  # in ms\n",
        "audio = AudioSegment.from_wav(audio_file)\n",
        "chunks = math.ceil(len(audio) / chunk_size)\n",
        "print(f\" Splitting into {chunks} chunks of 30s each...\")\n",
        "\n",
        "# 4. Process chunks with caching\n",
        "speaker_segments = []\n",
        "with tqdm(total=chunks, desc=\"Processing chunks\") as pbar:\n",
        "    for i in range(chunks):\n",
        "        start = i * chunk_size\n",
        "        end = min((i + 1) * chunk_size, len(audio))\n",
        "        chunk_file = f\"chunk_{i}.wav\"\n",
        "        cache_file = f\"chunk_{i}_diarization.pkl\"\n",
        "\n",
        "        # Export chunk only if not exists\n",
        "        if not os.path.exists(chunk_file):\n",
        "            audio[start:end].export(chunk_file, format=\"wav\")\n",
        "\n",
        "        # Load from cache if exists\n",
        "        if os.path.exists(cache_file):\n",
        "            with open(cache_file, \"rb\") as f:\n",
        "                diarization = pickle.load(f)\n",
        "        else:\n",
        "            # Run diarization on this chunk\n",
        "            diarization = pipeline(chunk_file)\n",
        "            with open(cache_file, \"wb\") as f:\n",
        "                pickle.dump(diarization, f)\n",
        "\n",
        "        # Extract speaker segments\n",
        "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "            speaker_segments.append({\n",
        "                \"speaker\": speaker,\n",
        "                \"start\": turn.start + start / 1000,  # shift time\n",
        "                \"end\": turn.end + start / 1000,\n",
        "                \"duration\": turn.end - turn.start\n",
        "            })\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "print(\" Diarization complete!\")\n",
        "print(\" Speakers found:\", set([s[\"speaker\"] for s in speaker_segments]))\n"
      ],
      "metadata": {
        "id": "Q0x2Vgv4EKds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "# Load Whisper model\n",
        "model = whisper.load_model(\"small\")   # small model = good balance (fast + accurate)\n",
        "\n",
        "# Transcribe audio\n",
        "result = model.transcribe(\"call.wav\")\n",
        "transcript = result[\"text\"]\n",
        "\n",
        "print(\" Transcription complete!\")\n",
        "print(\"Sample transcript:\", transcript[:250], \"...\")\n"
      ],
      "metadata": {
        "id": "4tg64H7oEST9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "# 1. Talk-time ratio\n",
        "talk_time = {}\n",
        "for seg in speaker_segments:\n",
        "    talk_time[seg[\"speaker\"]] = talk_time.get(seg[\"speaker\"], 0) + seg[\"duration\"]\n",
        "\n",
        "total_time = sum(talk_time.values())\n",
        "talk_ratios = {sp: round((dur/total_time)*100, 2) for sp, dur in talk_time.items()}\n",
        "\n",
        "# 2. Number of questions\n",
        "num_questions = transcript.count(\"?\")\n",
        "extra_questions = len(re.findall(r\"\\b(what|why|how|when|where|can|do|is|are|does|did)\\b\", transcript.lower()))\n",
        "question_count = max(num_questions, extra_questions)\n",
        "\n",
        "# 3. Longest monologue\n",
        "longest_monologue = max([seg[\"duration\"] for seg in speaker_segments])\n",
        "\n",
        "# 4. Sentiment\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "sentiment = sentiment_analyzer(transcript[:500])[0]  # analyze first 500 chars\n",
        "\n",
        "# 5. Actionable insight\n",
        "insight = \"\"\n",
        "if max(talk_ratios.values()) > 70:\n",
        "    insight = \"One speaker dominated the conversation. Allow more balanced talk-time.\"\n",
        "elif question_count < 3:\n",
        "    insight = \"Too few questions were asked. Ask more questions to engage the customer.\"\n",
        "else:\n",
        "    insight = \"Good balance, but could improve listening.\"\n",
        "\n",
        "# Identify roles\n",
        "sales_rep = max(talk_ratios, key=talk_ratios.get)\n",
        "customer = min(talk_ratios, key=talk_ratios.get)\n"
      ],
      "metadata": {
        "id": "4ndsDMo0H63n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n===== CALL QUALITY REPORT =====\")\n",
        "print(\"Talk-time ratio:\", talk_ratios)\n",
        "print(\"Questions asked:\", question_count)\n",
        "print(\"Longest monologue (sec):\", round(longest_monologue, 2))\n",
        "print(\"Call sentiment:\", sentiment[\"label\"], \"| Confidence:\", round(sentiment[\"score\"], 2))\n",
        "print(\"Actionable Insight:\", insight)\n",
        "print(\"Likely Sales Rep:\", sales_rep, \"| Likely Customer:\", customer)\n"
      ],
      "metadata": {
        "id": "EGhEi3hTIEPa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}