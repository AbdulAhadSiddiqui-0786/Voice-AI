{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOKtTs1NO2I1LxLWlqBgao",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdulAhadSiddiqui-0786/Voice-AI/blob/main/Voice_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KaeuccIyDLnB"
      },
      "outputs": [],
      "source": [
        "!pip install pytube pydub openai-whisper pyannote.audio torch torchvision torchaudio transformers -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp -q\n",
        "from pydub import AudioSegment\n",
        "\n",
        "url = \"https://www.youtube.com/watch?v=4ostqJD3Psc\"\n",
        "\n",
        "# Download audio with yt-dlp\n",
        "!yt-dlp -x --audio-format mp3 -o \"call.%(ext)s\" {url}\n",
        "\n",
        "# Convert to wav, mono, 16kHz\n",
        "audio = AudioSegment.from_file(\"call.mp3\")\n",
        "audio = audio.set_channels(1).set_frame_rate(16000)\n",
        "audio.export(\"call.wav\", format=\"wav\")\n",
        "\n",
        "print(\" Audio downloaded & converted to WAV using yt-dlp!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9cwj2BlDwhf",
        "outputId": "e8b86d0a-1854-4d40-b24c-db33421f6e1a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=4ostqJD3Psc\n",
            "[youtube] 4ostqJD3Psc: Downloading webpage\n",
            "[youtube] 4ostqJD3Psc: Downloading tv simply player API JSON\n",
            "[youtube] 4ostqJD3Psc: Downloading tv client config\n",
            "[youtube] 4ostqJD3Psc: Downloading tv player API JSON\n",
            "[youtube] 4ostqJD3Psc: Downloading player 0e6689e2-main\n",
            "[info] 4ostqJD3Psc: Downloading 1 format(s): 251\n",
            "[download] call.mp3 has already been downloaded\n",
            "[ExtractAudio] Not converting audio call.mp3; file is already in target format mp3\n",
            " Audio downloaded & converted to WAV using yt-dlp!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyannote.audio import Pipeline\n",
        "from google.colab import userdata\n",
        "from tqdm import tqdm\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "import math\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "#  Hugging Face token\n",
        "HUGGINGFACE_TOKEN = userdata.get('HUGGINGFACE_TOKEN')\n",
        "\n",
        "# 1. Load diarization model\n",
        "print(\" Loading diarization model...\")\n",
        "pipeline = Pipeline.from_pretrained(\n",
        "    \"pyannote/speaker-diarization\",\n",
        "    use_auth_token=HUGGINGFACE_TOKEN\n",
        ")\n",
        "print(\" Model loaded!\")\n",
        "\n",
        "# 2. Load audio and get duration\n",
        "audio_file = \"call.wav\"\n",
        "info = sf.info(audio_file)\n",
        "duration = info.duration  # in seconds\n",
        "print(f\" Audio duration: {duration:.1f} sec\")\n",
        "\n",
        "# 3. Split into 30s chunks\n",
        "chunk_size = 30_000  # in ms\n",
        "audio = AudioSegment.from_wav(audio_file)\n",
        "chunks = math.ceil(len(audio) / chunk_size)\n",
        "print(f\" Splitting into {chunks} chunks of 30s each...\")\n",
        "\n",
        "# 4. Process chunks with caching\n",
        "speaker_segments = []\n",
        "with tqdm(total=chunks, desc=\"Processing chunks\") as pbar:\n",
        "    for i in range(chunks):\n",
        "        start = i * chunk_size\n",
        "        end = min((i + 1) * chunk_size, len(audio))\n",
        "        chunk_file = f\"chunk_{i}.wav\"\n",
        "        cache_file = f\"chunk_{i}_diarization.pkl\"\n",
        "\n",
        "        # Export chunk only if not exists\n",
        "        if not os.path.exists(chunk_file):\n",
        "            audio[start:end].export(chunk_file, format=\"wav\")\n",
        "\n",
        "        # Load from cache if exists\n",
        "        if os.path.exists(cache_file):\n",
        "            with open(cache_file, \"rb\") as f:\n",
        "                diarization = pickle.load(f)\n",
        "        else:\n",
        "            # Run diarization on this chunk\n",
        "            diarization = pipeline(chunk_file)\n",
        "            with open(cache_file, \"wb\") as f:\n",
        "                pickle.dump(diarization, f)\n",
        "\n",
        "        # Extract speaker segments\n",
        "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "            speaker_segments.append({\n",
        "                \"speaker\": speaker,\n",
        "                \"start\": turn.start + start / 1000,  # shift time\n",
        "                \"end\": turn.end + start / 1000,\n",
        "                \"duration\": turn.end - turn.start\n",
        "            })\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "print(\" Diarization complete!\")\n",
        "print(\" Speakers found:\", set([s[\"speaker\"] for s in speaker_segments]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0x2Vgv4EKds",
        "outputId": "e75cd7f7-0edc-4198-f4e3-22bcdd15d77b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loading diarization model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n",
            "/usr/local/lib/python3.12/dist-packages/pyannote/audio/core/io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  torchaudio.list_audio_backends()\n",
            "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/hyperparams.yaml'\n",
            "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.8.0+cu126. Bad things might happen unless you revert torch to 1.x.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:speechbrain.utils.parameter_transfer:Collecting files (or symlinks) for pretraining in /root/.cache/torch/pyannote/speechbrain.\n",
            "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/embedding_model.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"embedding_model\"] = /root/.cache/torch/pyannote/speechbrain/embedding_model.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"mean_var_norm_emb\"] = /root/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/classifier.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"classifier\"] = /root/.cache/torch/pyannote/speechbrain/classifier.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Using symlink found at '/root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"label_encoder\"] = /root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt\n",
            "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): embedding_model -> /root/.cache/torch/pyannote/speechbrain/embedding_model.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): mean_var_norm_emb -> /root/.cache/torch/pyannote/speechbrain/mean_var_norm_emb.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): classifier -> /root/.cache/torch/pyannote/speechbrain/classifier.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): label_encoder -> /root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt\n",
            "DEBUG:speechbrain.dataio.encoder:Loaded categorical encoding from /root/.cache/torch/pyannote/speechbrain/label_encoder.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model loaded!\n",
            " Audio duration: 122.7 sec\n",
            " Splitting into 5 chunks of 30s each...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing chunks: 100%|██████████| 5/5 [00:00<00:00, 3439.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Diarization complete!\n",
            " Speakers found: {'SPEAKER_03', 'SPEAKER_01', 'SPEAKER_00', 'SPEAKER_02', 'SPEAKER_04'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "# Load Whisper model\n",
        "model = whisper.load_model(\"small\")   # small model = good balance (fast + accurate)\n",
        "\n",
        "# Transcribe audio\n",
        "result = model.transcribe(\"call.wav\")\n",
        "transcript = result[\"text\"]\n",
        "\n",
        "print(\" Transcription complete!\")\n",
        "print(\"Sample transcript:\", transcript[:250], \"...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tg64H7oEST9",
        "outputId": "c390903f-e4b2-4338-b2fe-34096b50905d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "# 1. Talk-time ratio\n",
        "talk_time = {}\n",
        "for seg in speaker_segments:\n",
        "    talk_time[seg[\"speaker\"]] = talk_time.get(seg[\"speaker\"], 0) + seg[\"duration\"]\n",
        "\n",
        "total_time = sum(talk_time.values())\n",
        "talk_ratios = {sp: round((dur/total_time)*100, 2) for sp, dur in talk_time.items()}\n",
        "\n",
        "# 2. Number of questions\n",
        "num_questions = transcript.count(\"?\")\n",
        "extra_questions = len(re.findall(r\"\\b(what|why|how|when|where|can|do|is|are|does|did)\\b\", transcript.lower()))\n",
        "question_count = max(num_questions, extra_questions)\n",
        "\n",
        "# 3. Longest monologue\n",
        "longest_monologue = max([seg[\"duration\"] for seg in speaker_segments])\n",
        "\n",
        "# 4. Sentiment\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "sentiment = sentiment_analyzer(transcript[:500])[0]  # analyze first 500 chars\n",
        "\n",
        "# 5. Actionable insight\n",
        "insight = \"\"\n",
        "if max(talk_ratios.values()) > 70:\n",
        "    insight = \"One speaker dominated the conversation. Allow more balanced talk-time.\"\n",
        "elif question_count < 3:\n",
        "    insight = \"Too few questions were asked. Ask more questions to engage the customer.\"\n",
        "else:\n",
        "    insight = \"Good balance, but could improve listening.\"\n",
        "\n",
        "# Identify roles\n",
        "sales_rep = max(talk_ratios, key=talk_ratios.get)\n",
        "customer = min(talk_ratios, key=talk_ratios.get)\n"
      ],
      "metadata": {
        "id": "4ndsDMo0H63n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n===== CALL QUALITY REPORT =====\")\n",
        "print(\"Talk-time ratio:\", talk_ratios)\n",
        "print(\"Questions asked:\", question_count)\n",
        "print(\"Longest monologue (sec):\", round(longest_monologue, 2))\n",
        "print(\"Call sentiment:\", sentiment[\"label\"], \"| Confidence:\", round(sentiment[\"score\"], 2))\n",
        "print(\"Actionable Insight:\", insight)\n",
        "print(\"Likely Sales Rep:\", sales_rep, \"| Likely Customer:\", customer)\n"
      ],
      "metadata": {
        "id": "EGhEi3hTIEPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5RkN0UJQ8R7t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}